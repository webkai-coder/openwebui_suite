# OpenWebUI

Lokal lauffähige Oberfläche für LLMs (z. B. Ollama, lokale Mistral-Modelle, GPT-Gateways).
Start:
```bash
docker compose up -d
